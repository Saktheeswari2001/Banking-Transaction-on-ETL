from pyspark.sql.functions import broadcast

joined_df = transactions_df.join(broadcast(customers_df), on="customer_id", how="inner")
display(joined_df)



from pyspark.sql.functions import sum

aggregated_df = joined_df.groupBy("customer_id", "type").agg(sum("amount").alias("total_spent"))
display(aggregated_df)

final_df = aggregated_df.join(broadcast(customers_df.select("customer_id", "state")), on="customer_id")
final_df.write.mode("overwrite").partitionBy("state").parquet("output/customer_spend_summary")
final_df.show()
